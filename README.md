# Multimodal-Activity-Recognition-using-Deep-learning
Academic project for constructing deep learning models for human activity recognition by using different modalities namely gaze and egocentric images.


ABSTRACT

Human visual behaviour is a very rich source of information to understand human activities. The combination of this visual behaviour with scene information could significantly boost the performance of recognizing activities. Previous works have focused on using both supervised and unsupervised methods for determination of predefined activities only from short term and long term visual behaviour. In our work, we integrate two different modalities namely visual gaze behaviour and visual scene information modals using late fusion technique, provide a comparison on three different deep learning architectures with a combination of CNN and LSTM for activity recognition by using long term gaze information and egocentric view based visual scene information. Our results show that an architecture with LSTM and pre-trained VGG19 outperformed the other two models.


This Acedemic project was guided by Andreas Bulling, Huy Viet Le, Sven Mayer from University of Stuttgart.
